{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs were explicitly designed to solve the vanishing gradient problem that we previously saw.  As mentioned, a vanishing gradient occurs when our gradient gets drastically smaller as we backpropagate further.  And RNNs are particularly susceptible to vanishing gradients as the hypothesis function repeatedly uses the same weight matrix $W_h$ to calculate the next hidden state.  Because of this, when backpropagating, we repeatedly multiply by $W_h$ for each time step earlier than $t$.  And this means that  when predicting the next word, $w_{t+1}$, our RNNs tend not to incorporate words from earlier timesteps, $w_{t-n}$.  \n",
    "\n",
    "In this lesson, we'll read through chris olah's blog post, where he explains some of the intuition behind LSTMs and how they help to solve the vanishing gradient problem in RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blog Post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through this [blog post on LSTMS](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
