{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Vanishing Gradient Problem\n",
    "\n",
    "The vanishing gradient problem is the following:\n",
    "> When determining how to update our parameters through backpropagation, if a local gradient is smaller, this gradient gets drastically smaller as we backpropagate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanishing gradient problem can occur in feedforward neural networks, and convolutional neural networks, that we have seen so far.  But with RNNs, we are particularly susceptible to the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs and Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall that with an RNN, the key component is that we are repeatedly calculating a hidden state, where that hidden state is a function both of the previous hidden state, our current word embedding, and our weights and biases $W_h$, $W_e$, and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(t) = \\text{tanh}(W_h h^{t -1} + W_e e^t + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we'll take that final hidden state, and make a prediction as to the next word, and from there determine how to update our parameters after calculating how off our prediction was.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things more concrete, let's say that we are predicting the fifth word, so we make a hypothesis at step 4, and this means that we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $h_4 = \\text{tanh}(W_h h^{3} + W_xx^4 + b)$\n",
    "\n",
    "And that $J_4(\\theta) = J_4(w_5, h_4) $ is a function of $h_4$.  Now, as we know we have some loss $J_5$, and we then want to update our parameters to reduce our loss.  Currently, it looks like our parameters that predict $w_5$ are limited to what we see above us: $W_h, h^{3}, W_x, x^4$, but really this $h_3$ is a function of arises as a function of all of the previous hidden states.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of this visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"h1-with-words.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, our prediction of the next word is (rightfully) influenced by the preceding words through our hidden states.  And let's say we want to determine how to update $h_1$ so that we reduce our cost $J_4(\\theta)$.  \n",
    "\n",
    "> Or in other words, we want to change how the word \"dog\" influences our prediction of the word after \"over the\", to reduce our cost $J_4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we need to calculate $\\frac{\\delta J_4(\\theta)}{h1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Really technical note**: While we cannot directly change $h_1$, we **can** change $W_h$ which influences $h_1$.  So to determine how to update $W_h$ for the purposes of changing $h_1$, we need to determine $h_1$'s influence on $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating the influence of $h_1$ on $J_4$ becomes a problem, because this equals: \n",
    "\n",
    "$\\frac{\\delta J_4(\\theta)}{h1} = \\frac{\\delta J_4(\\theta)}{h_4}*W_h*W_h*W_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point is that we are repeatedly multiplying by $W_T$.  \n",
    "\n",
    "> If your curious, we'll show how we get there later on.  But for right now, let's just assume this is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is where our vanishing gradient comes in.  Let's see how.  Let's set $W_h$ to be the following matrix, and see what occurs as we multiply $W_h*W_h*W_h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1295 , 0.00753],\n",
       "       [0.2259 , 0.1295 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "W_h = np.array([[.5, .01], [.3, .5]])\n",
    "W_h.dot(W_h).dot(W_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we can see is that the further more we multiply by $W_t$, the smaller our numbers get.  And remember, this is the vanishing gradient problem:\n",
    "\n",
    "> When determining how to update our parameters through backpropagation, if a local gradient is smaller, this gradient gets drastically smaller as we backpropagate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it's a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is a problem because oftentimes words earlier on can have a strong influence later down in the sequence.  For example, we saw the influence of the word dog in our current example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"h1-with-words.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the problem can be way more extreme then just a few words.  For example consider one of our Amazon reviews:\n",
    "\n",
    "> This is my favorite coconut water that I have tried.  I have tried a lot of different coconut water, and this is the _____.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction that the next word has a strong chance of being \"best\" is influenced by the word favorite earlier on.  But because the words are 19 steps apart, it would have a small influence in our current formulation of RNNs.\n",
    "\n",
    "Another thing to point out is that a consequence of the vanishing gradient problem is not just that the influence of earlier words is small, but that the influence is drastically smaller than more recent words.  And as we viewed in our example, it's the earlier words that can be more predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little deeper into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we previously just skipped to the end in specifying that our derivative  $\\frac{\\delta J_4(\\theta)}{h1} = \\frac{\\delta J_4(\\theta)}{h_4}*W_h*W_h*W_h$.  Let's see why this is the case.\n",
    "\n",
    "Remember when we find $\\frac{\\delta J_4(\\theta)}{h1}$, we are trying to calculate the influence of our hidden state at time 1 on our cost function at time 4.  As we see from the diagram below, the influence of $h1$ on $J_4$ is indirect, and thus we'll need the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./backprop-rnn1.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we need to calculate how a nudge of $h_1$ influences $h_2$, and then how a nudge of $h_2$ influences $h_3$ and so on.  We can write this out as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} \\frac{\\delta J_4}{\\delta h_4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate those three derivatives, $\\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} $ individually with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* $h_4 = W_h h_3 + W_x x_4 +b \\rightarrow \\frac{\\delta h_4}{\\delta h_3} = W_h$ \n",
    "* $h_3 = W_h h_2 + W_x x_3 + b  \\rightarrow \\frac{\\delta h_3}{\\delta h_2} = W_h$ \n",
    "* $h_2 = W_h h_1 + W_x x_3 + b \\rightarrow \\frac{\\delta h_2}{\\delta h_1} = W_h$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that $\\frac{\\delta J_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} \\frac{\\delta J_4}{\\delta h_4} = W_h*W_h*W_h \\frac{\\delta J_4}{\\delta h_4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see occurring is that because: \n",
    "\n",
    "$\\frac{\\delta h_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, if we ask the question, how should $h_1$ be nudged to have an influence on $h_4$, so that we ultimately reduce $J_4(\\theta)$, we see that  $\\frac{\\delta h_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} = W_h*W_h*W_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./vanishing-grad-intuition.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the gradient of loss of J on step $i$, with respect to the hidden state on some previous step $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because at each step, we multiply by the weight matrix, for the chain rule, we keep multiplying the the Weight matrix.\n",
    "\n",
    "And $W_h$ is small then the term gets vanishingly small as current step $i$ and hidden state $j$ are further apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also if greater than one, will exponentially explode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why vanishing gradients are bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./vanishing-grad-problem.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So gradient signal from faraway is lost.  And sometimes we do want to learn the connection between what happens early and what happens later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Also, gradient can be viewed as measure of effect of past on the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, remember our language model problem:\n",
    "\n",
    "* When she tried to print tickets, found it was out of toner.  But she finally printed her tickets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if the gradient is small, the model can't learn this dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The writer of the books is\n",
    "\n",
    "* Syntactic recency (writer, is)\n",
    "* Sequential recency (just how close words are)\n",
    "And if mostly only paying attention to recent.  And RNNs are better at learning from sequential recency then syntactic recency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploding gradient is a problem because are then changing the parameters too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "\n",
    "1.For exploding gradient, gradient clipping\n",
    "* If norm of gradient is greater than some threshold, then scale it down before applying the SGD update.\n",
    "* Intuition, still take in same direction, but a smaller step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. If vanishing gradient\n",
    "\n",
    "* Problem: Too difficult for RNN to learn to preserve information over many time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In vanilla RNN, hidden state is constantly being re-written.  In particular because of this non-linearity function.  So not easy to preserve information from one hidden state to another.\n",
    "\n",
    "So what about an RNN with a separate memory?  Then could it learn to preserve information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "* An RNN to solve the vanishing gradient problem.\n",
    "* So now we have both a hidden state and a cell state.  \n",
    "    * Both have vectors of length n\n",
    "    * The cell stores the long term information\n",
    "    * The LSTM can erase, write and read information from the cell\n",
    "    \n",
    "* The selection of which information is erased, written or read is controlled by gates.\n",
    "    * The gates are also vectors of length n\n",
    "    * And each element of the gates are btwn 0 and 1, where 1 is open and 0 is closed\n",
    "    * So if if gate open then info passed through, and if not\n",
    "* the gates are dynamic - their value is computed based on the current context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formula\n",
    "\n",
    "On timestep t\n",
    "\n",
    "* f^t = \\sigma()\n",
    "* Forget gate: controls what is kept vs forgotten from previous cell state (previous hidden state and current $x_t$\n",
    "* Input gate - what part of the new cell content are written to the memory cell\n",
    "* Output gate: What parts of the cell are outputs to the hidden state (this is the read function, and then would be put into the hidden state).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c^t: what we write to the cell\n",
    "* f^t we decide to forget some information\n",
    "\n",
    "Finally, pass cell through output gate and tanh to give us the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the cell is an internal memory, and the hidden state is what we pass on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"olah-lstm-diagram.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./lstm-formulas.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of LSTMs\n",
    "\n",
    "LSTMS make it easier to preserve over many time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the architecture makes it easier to preserve information over many timesteps.\n",
    "\n",
    "* If forget gate remembers everything on every step, then cell preserved indefinitely.\n",
    "\n",
    "### Vanishing/exploding gradient\n",
    "\n",
    "* This is a problem for all neural architectures, especially deep ones.\n",
    "* Though vanishing/exploding gradients are a general problem, RNNs are particularly unstable, due to the repeated multiplication by the same weight matrix.\n",
    "* Remember, we are applying the same weight matrix again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
