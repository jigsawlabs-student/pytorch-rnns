{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancements to LSTMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll learn about a couple of enhancements that we can make to our RNNs or LSTMs.  The first is a bidirectional RNN, and the second is stacking layers of our RNN.  Let's cover each of these in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that with RNNs, we predict each subsequent word by looking at the words that came before it.  These predictions lead to a hidden state, or collection of our hidden states which we think of as encoding our document, and we can then  pass this hidden state into a linear layer, to ultimately classify our document.\n",
    "\n",
    "Now because we are using our hidden state to encode our document, we obviously want it to perform as well as possible.  Another way of thinking about it, is we want our hidden state to capture the *context* that each word appears in.  \n",
    "\n",
    "> For this reason a hidden state is oftentimes called a contextual representation.\n",
    "\n",
    "Now, one issue with our current approach is that we are only considering *earlier* words, $w_{t - n}$, and not later words in capturing the context of our words.  Take for example, the following phrase:\n",
    "\n",
    "> \"I need to buy a _ _ _ _ _  \n",
    "\n",
    "> before going on the train.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can hopefully see from the above, is that a lot of the context for the blank word does not come from the earlier words in the document, but the later words.  One fix for this, is instead of only generating a hidden state based on the previous words, we also generate a hidden state by starting from the end of the document and moving backwards.  This way when we get to the blank word, we are including the word \"train\" in our context.\n",
    "\n",
    "> Generating the hidden state by moving forwards through each word in the document is called a forwards rnn.  And moving backwards through the document is called a backwards rnn.  When we capture both hidden states, it's called a **bidirectional rnn**.  \n",
    "\n",
    "Let's see how we can capture both the context of previous words and later words in our hidden state with the following diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./bidirectional-rnn.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see from the above, is that one hidden state -- in the forwards row -- is trained to predict subsequent words.  And the other hidden state -- in the backwards row -- is trained to predict earlier words.  And what is passed on is a concatenation of the two hidden states.  This way the neural network can incorporate both the fowards and backwards context in it's representation of a document.\n",
    "\n",
    "> Note that a bidirectional RNN is not applicable to language modeling because in language modeling we're predicting what word comes next, so this would result in data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer RNNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multilayer RNN, also called a stacked RNN, works similarly to the multilayer neural networks we saw previously.  With a multilayer RNN, the first layer takes in our raw data, here our word embeddings and combines this input with the previous hidden state to produce the next hidden state.  \n",
    "\n",
    "At the second level, instead of passing in the word embedding at that time $t$, we pass through the hidden state at that time from the previous layer.  So to calculate our hidden state in the second column, $t_12$ and the second layer, we get $h_{22} = W_{h1}h_{12} + W_{h}h_{21}$.  Take a look at the diagram below to see this more clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"multilayer-h1.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea with a multilayer RNN is similar to that in our CNN or feedforward neural networks.  That our neural network will find more abstract features with deeper layers.  So here, the lower RNNs might focus on features such as syntax, and higher RNNs would extract more of the underlying meaning in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "[Transformers](https://www.youtube.com/watch?v=5vcj8kSwBCY&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
