{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll work through building the hypothesis function for a recurrent neural network in Python.  In doing so, we'll explore how an RNN generates and uses hidden state for each word in a document.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Initial Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at our diagram for a recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Rnn-diagram-no-pred.png\"  width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we begin with an index that represents each word.  Then from the index we get the related embedding.  Then we produce a hidden state by taking the embedding and multiplying by weights, and getting the previous hidden state and also multiplying by weights.  We'll combine the two computations with addition.  Ok, let's start implementing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by loading up some libraries, and let's initialize a word vector through the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7056],\n",
       "        [ 0.6741],\n",
       "        [-0.5454],\n",
       "        [ 0.9107]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(15)\n",
    "e_1 = torch.randn(4, 1)\n",
    "e_1\n",
    "# dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also initialize an inital hidden state $h_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "h_0 = torch.randn(1, 1)\n",
    "h_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we know, we'll produce the next hidden state with the function $H_t = e_t \\cdot w_e +  h_{t-1}\\cdot w_h$.  So we'll need weights to associate with both the embedding and the hidden state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2138]]),\n",
       " tensor([[-1.3780],\n",
       "         [-0.0546],\n",
       "         [ 0.4515],\n",
       "         [ 0.7858]]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(12)\n",
    "w_h = torch.randn(1, 1)\n",
    "w_e = torch.randn(4, 1)\n",
    "w_h, w_e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get to the next hidden state by multiplying the embedding and the hidden state by their respective weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4288]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(h_0, w_h) + torch.mm(e_1.T, w_e)\n",
    "#        1x1, 1x1            1x4.    4x1 = 1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now move the operation above, so we can reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_t(e_t, w_e, h_t_prev, w_h):\n",
    "    return torch.mm(h_t_prev, w_h) + torch.mm(e_t.T, w_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4288]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t(e_1, w_e, h0, w_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we now have our formula to calculate the next hidden state.  Now let's take a sequence of word embeddings, and predict the next hidden state for each word embedding, $e_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3029,  0.2045, -0.9202, -0.8919,  0.2516],\n",
       "        [ 0.9675, -0.6870,  0.9042,  0.3286, -0.0742],\n",
       "        [ 0.1414, -1.2538, -0.3456, -0.2211, -0.7043],\n",
       "        [ 0.3368,  0.0064,  0.2326,  0.9527, -0.4139]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dog jumped over a\n",
    "E = torch.randn(4, 5)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5197]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(e_t.T.unsqueeze(0), w_e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = [h_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e_t in E.T:\n",
    "    e_t = e_t.T.unsqueeze(0)\n",
    "    \n",
    "    prev_hidden = hidden_states[-1]\n",
    "    hidden_prod = torch.mm(prev_hidden, w_h)\n",
    "    embed_prod = torch.mm(e_t, w_e)\n",
    "    new_h = hidden_prod + embed_prod\n",
    "    hidden_states.append(new_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1117]]),\n",
       " tensor([[-1.4958]]),\n",
       " tensor([[-0.4856]]),\n",
       " tensor([[1.3492]]),\n",
       " tensor([[1.5714]]),\n",
       " tensor([[-1.3220]])]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are six total hidden states.  There's one for each of the words, $e_1 ... e_t$, and plus the initial hidden state $h_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Oriented RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_states = [torch.randn(1, 1)]\n",
    "        self.We = torch.randn(n_inputs, n_neurons) # 4 X 1\n",
    "        self.Wh = torch.randn(n_neurons, n_neurons) # 1 X 1\n",
    "    \n",
    "    def forward(self, e):\n",
    "        embed_mult = torch.tanh(torch.mm(e, self.We)) # 4 X 1\n",
    "        prev_hidden = self.hidden_states[-1]\n",
    "        hidden_mult = torch.mm(prev_hidden, self.Wh) # 4 X 1\n",
    "        hidden_state = torch.tanh(hidden_mult + embed_mult)\n",
    "        self.hidden_states.append(hidden_state)\n",
    "        return hidden_state\n",
    "        #1x4 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_states = [torch.randn(1, n_neurons)]\n",
    "        self.We = nn.Linear(n_inputs, n_neurons) # 4 X 1\n",
    "        self.Wh = nn.Linear(n_neurons, n_neurons) # 1 X 1\n",
    "    \n",
    "    def forward(self, e):\n",
    "        \n",
    "        prev_hidden = self.hidden_states[-1]\n",
    "        \n",
    "        hidden_mult = self.Wh(prev_hidden) \n",
    "        embed_mult = self.We(e) # 4 X 1\n",
    "        hidden_state = torch.tanh(hidden_mult + embed_mult)\n",
    "        self.hidden_states.append(hidden_state)\n",
    "        return hidden_state\n",
    "        #1x4 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (We): Linear(in_features=4, out_features=1, bias=True)\n",
       "  (Wh): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(4, 1)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e_t in E.T:\n",
    "    rnn(e_t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.1709]]),\n",
       " tensor([[-0.0148]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.0774]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.1840]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.2288]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.3159]], grad_fn=<TanhBackward>)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now currrently we are setting up our rnn so that our hidden state is a single number.  But we can increase the number of neurons on our W_h, and increase the number of columsn in our hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (We): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (Wh): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(4, 2)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e_t in E.T:\n",
    "    rnn(e_t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.8746, -1.1934]]),\n",
       " tensor([[0.7477, 0.7169]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.7183, 0.0297]], grad_fn=<TanhBackward>),\n",
       " tensor([[-0.0921, -0.0526]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.3248, 0.5440]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.6136, 0.2024]], grad_fn=<TanhBackward>)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[RNN Pytorch](https://medium.com/dair-ai/building-rnns-is-fun-with-pytorch-and-google-colab-3903ea9a3a79)\n",
    "\n",
    "[NN from Scratch Pytorch](https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
