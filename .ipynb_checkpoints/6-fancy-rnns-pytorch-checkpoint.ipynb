{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll work through how to implement LSTMs in Pytorch.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As a first step, let's change the runtime type in colab to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by downloading our data.  As usual, we set up seeding for Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "SEED = 12\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that this time, we included an argument `include_lengths = True`.  This will come in handy later.  Let's keep going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we download our IMDB data and build our vocabulary, and batch our data into sizes of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = 25_000, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = 64,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we select the first batch of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    first_batch_text = batch.text\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to construct our LSTM model.  This is the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(25002, 100, padding_idx = 1)\n",
    "        self.lstm_layer = nn.LSTM(100, 256, num_layers=2, bidirectional=True, \n",
    "                           dropout=.5)\n",
    "        self.fc = nn.Linear(256 * 2, 1)\n",
    "    def forward(self, text, sentence_lengths):\n",
    "        return self.embedding(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's really only three layers that we'll need: an embedding layer, our lstm, and our linear layer, which is our ouput layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that our forward function is not flushed out right now.  It simply takes in text and returns the output from the embedding layer.  We'll fill it in as we move along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let's initialize and copy over the embeddings.  Then we'll go through each layer of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "lstm.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "lstm.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "lstm.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's take a look at the layers in our LSTM model.  We'll go through each of these layers in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
       "  (lstm): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Embedding Layer\n",
    "\n",
    "* In the embedding layer, we specify the number of rows, one for each word in the dictionary.  And the number of columns is the length of our vectors, 100.  \n",
    "\n",
    "* When we pass the padding index, we tell Pytorch not to update this vector during training, as we do not believe that padding can help us predict the sentiment of a review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LSTM Layer\n",
    "\n",
    "```python\n",
    "nn.LSTM(100, 256, num_layers=2, bidirectional=True, dropout=.5)\n",
    "```\n",
    "\n",
    "* Because the output from our preceding *embedding* layer is a vector of length 100 for each word, we specify 100 rows for the weight matrix of our LSTM.  The number of columns that we specify determines the size of the hidden dimension.  Here we set it to 256.  \n",
    "* We can make each layer bidirectional with our `bidirectional = True`, and because this concatenates the two hidden states, our combined hidden state is really of length 512.  \n",
    "* Finally, we specify two layers for our LSTM.  One layer passing the hidden state to the corresponding column in the subsequent level.  And between each layer we hae a dropout of `.5`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Output layer\n",
    "\n",
    "```python \n",
    "nn.Linear(256 * 2, 1)\n",
    "```\n",
    "\n",
    "Notice that the output layer is takes in `256*2` features, and outputs a single neuron.  This is because the input from the preceding layer is a hidden state from a bidirectional RNN, where the forward and backwards hidden state is each of length 256.  And then want a single output to represent whether our sentence is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at how we initialize our layers before moving on.  Try to feel comfortable with each of the parameters we are initializing our different layers with.  And the task that each layer is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(25002, 100, padding_idx = 1)\n",
    "        self.lstm_layer = nn.LSTM(100, 256, num_layers=2, bidirectional=True, \n",
    "                           dropout=.5)\n",
    "        self.fc = nn.Linear(256 * 2, 1)\n",
    "    def forward(self, text, document_lengths):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we initialized each of our layers, it's time to move through the building our the forward function.  \n",
    "\n",
    "1. Document lengths\n",
    "\n",
    "Looking at the forward function, notice that we pass through both the numericalized text of each document, and the `document_lengths` as arguments.\n",
    "\n",
    "The `document_lengths` are a list of numbers signifying the number of words in each document, not including the padding words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because we specified `include_lengths` when tokenizing, `TEXT = data.Field(tokenize = 'spacy', include_lengths = True)`, pytorch returns the non-padded lengths to us.  It's the second element of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([139, 115,  60])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_document_lengths = first_batch_text[1]\n",
    "batch_document_lengths[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, above, we see the document lengths for the last three documents in our batch.  We'll eventually use these document_lengths so that when we pass each document to our LSTM, we can remove the padding tokens, which we do not believe to effect sentiment, and so we do not want captured in the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing that, let's select the first element from our text, our batch of numericalized documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_text = first_batch_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([713, 64])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The embedding layer and document packing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now it's time to pass our data through the layer.  We can fill in the first two lines of the forward function with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_batch = lstm.embedding(batch_text) # torch.Size([713, 64, 100])\n",
    "packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded_batch, \n",
    "                                                    batch_document_lengths, \n",
    "                                                    enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first line we pass the numericalized text through our embedding layer, which returns a corresponding embedding vector for each word.  Then, in the second line, we call `pack_padded_sequence` to remove the paddings from each document before we pass our documents through the LSTM layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is now a 100 length vector, for each non-padded word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16470, 100])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_embedded.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16470)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(document_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The LSTM layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have \"packed\" versions of each of our documents, we can pass our documents to our lstm layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_output, (hidden, cell) = lstm.lstm_layer(packed_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns to us three different components.  The first is the `packed_component`, which represents the hidden state across all tokens, of the last layer of our LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16470, 512])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_output.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we have the output from the 512 neurons for each of the non-padded words in our batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second is hidden, which is the hidden state of only the last words in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 256])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the first dimension is a 4 as hidden contains the last hidden output from both of the layers in our LSTM, both the forward and backwards outputs.    \n",
    "\n",
    "We only need the outputs from the last layer, the last two matrices, so we can select that with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_forwards = hidden[-2,:,:]\n",
    "last_forwards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_backwards = hidden[-1, :, :]\n",
    "last_backwards.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is a final state for each of the 64 documents in the batch, and the hidden state is of length 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Passing the hidden states to the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can concatenate the forwards and backwards hidden states together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_hidden = torch.cat((last_forwards, last_backwards), dim = 1)\n",
    "combined_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then pass this to our output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fc(combined_hidden).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we walked through each of the steps, let's update our neural network by filling in our `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(25002, 100, padding_idx = 1)\n",
    "        self.lstm_layer = nn.LSTM(100, 256, num_layers=2, bidirectional=True, \n",
    "                           dropout=.5)\n",
    "        self.fc = nn.Linear(256 * 2, 1)\n",
    "    def forward(self, text, document_lengths):\n",
    "        embedded_batch = self.embedding(text) # torch.Size([713, 64, 100])\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded_batch, \n",
    "                                                    document_lengths, \n",
    "                                                    enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm_layer(packed_embedded)\n",
    "        l2_forwards = hidden[-2,:,:]\n",
    "        l2_backwards = hidden[-1, :, :]\n",
    "        combined_hidden = torch.cat((l2_forwards, \n",
    "                                     l2_backwards), dim = 1)\n",
    "        return self.fc(combined_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final version of our LSTM, so take a look at it to ensure that each of the components make sense.\n",
    "\n",
    "Then let's initialize an instance of the LSTM, and see how we can pass through some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "updated_lstm.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "updated_lstm.embedding.weight.data[UNK_IDX] = torch.zeros(100)\n",
    "updated_lstm.embedding.weight.data[PAD_IDX] = torch.zeros(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember that each batch, is a tuple of both numericalized documents and the document lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can pass them to our LSTM individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_lstm = updated_lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = updated_lstm(first_batch[0], first_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use `splat` to have Python unpack the tuple, and pass element through as separate elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = updated_lstm(*first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now it's time to train the model.  We initialize our optimizer, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "bce_loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call `to(device)` so that our model is trained with cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_lstm = updated_lstm.to(device)\n",
    "bce_loss = bce_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(7):\n",
    "    for batch in train_iterator:\n",
    "        preds = updated_lstm(batch.text[0].cuda(), batch.text[1].cuda())\n",
    "        loss = bce_loss(preds.squeeze(1), batch.label.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we can compute the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_lstm.eval()\n",
    "\n",
    "accuracies = []\n",
    "batch_lengths = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        outputs = updated_lstm(*batch.text)\n",
    "        labels = batch.label\n",
    "        accuracy = binary_accuracy(outputs.squeeze(1), labels)\n",
    "        accuracies.append(accuracy.item())\n",
    "        batch_lengths.append(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([accuracy*batch_length for accuracy, batch_length in zip(accuracies, batch_lengths)])/sum(batch_lengths)\n",
    "# .878"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
