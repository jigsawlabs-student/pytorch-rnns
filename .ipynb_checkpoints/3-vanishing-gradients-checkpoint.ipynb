{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Vanishing Gradient Problem\n",
    "\n",
    "The vanishing gradient problem is the following:\n",
    "> When determining how to update our parameters through backpropagation, if a local gradient is smaller, this gradient gets drastically smaller as we backpropagate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanishing gradient problem can occur in feedforward neural networks, and convolutional neural networks, that we have seen so far.  But with RNNs, we are particularly susceptible to the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs and Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall that with an RNN, the key component is that we are repeatedly calculating a hidden state, where that hidden state is a function both of the previous hidden state, our current word embedding, and our weights and biases $W_h$, $W_e$, and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(t) = \\text{tanh}(W_h h^{t -1} + W_e e^t + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we'll take that final hidden state, and make a prediction as to the next word, and from there determine how to update our parameters after calculating how off our prediction was.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things more concrete, let's say that we are predicting the fifth word, so we make a hypothesis at step 4, and this means that we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $h_4 = \\text{tanh}(W_h h^{3} + W_xx^4 + b)$\n",
    "\n",
    "And that $J_4(\\theta) = J_4(w_5, h_4) $ is a function of $h_4$.  Now, as we know we have some loss $J_5$, and we then want to update our parameters to reduce our loss.  Currently, it looks like our parameters that predict $w_5$ are limited to what we see above us: $W_h, h^{3}, W_x, x^4$, but really this $h_3$ is a function of arises as a function of all of the previous hidden states.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of this visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"h1-with-words.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, our prediction of the next word is (rightfully) influenced by the preceding words through our hidden states.  And let's say we want to determine how to update $h_1$ so that we reduce our cost $J_4(\\theta)$.  \n",
    "\n",
    "> Or in other words, we want to change how the word \"dog\" influences our prediction of the word after \"over the\", to reduce our cost $J_4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we need to calculate $\\frac{\\delta J_4(\\theta)}{h1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Really technical note**: While we cannot directly change $h_1$, we **can** change $W_h$ which influences $h_1$.  So to determine how to update $W_h$ for the purposes of changing $h_1$, we need to determine $h_1$'s influence on $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating the influence of $h_1$ on $J_4$ becomes a problem, because this equals: \n",
    "\n",
    "$\\frac{\\delta J_4(\\theta)}{h1} = \\frac{\\delta J_4(\\theta)}{h_4}*W_h*W_h*W_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point is that we are repeatedly multiplying by $W_T$.  \n",
    "\n",
    "> If your curious, we'll show how we get there later on.  But for right now, let's just assume this is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is where our vanishing gradient comes in.  Let's see how.  Let's set $W_h$ to be the following matrix, and see what occurs as we multiply $W_h*W_h*W_h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1295 , 0.00753],\n",
       "       [0.2259 , 0.1295 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "W_h = np.array([[.5, .01], [.3, .5]])\n",
    "W_h.dot(W_h).dot(W_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we can see is that the further more we multiply by $W_t$, the smaller our numbers get.  And remember, this is the vanishing gradient problem:\n",
    "\n",
    "> When determining how to update our parameters through backpropagation, if a local gradient is smaller, this gradient gets drastically smaller as we backpropagate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it's a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is a problem because oftentimes words earlier on can have a strong influence later down in the sequence.  For example, we saw the influence of the word dog in our current example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"h1-with-words.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the problem can be way more extreme then just a few words.  For example consider one of our Amazon reviews:\n",
    "\n",
    "> This is my favorite coconut water that I have tried.  I have tried a lot of different coconut water, and this is the _____.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction that the next word has a strong chance of being \"best\" is influenced by the word favorite earlier on.  But because the words are 19 steps apart, it would have a small influence in our current formulation of RNNs.\n",
    "\n",
    "Another thing to point out is that a consequence of the vanishing gradient problem is not just that the influence of earlier words is small, but that the influence is drastically smaller than more recent words.  And as we viewed in our example, it's the earlier words that can be more predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little deeper into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we previously just skipped to the end in specifying that our derivative  $\\frac{\\delta J_4(\\theta)}{h1} = \\frac{\\delta J_4(\\theta)}{h_4}*W_h*W_h*W_h$.  Let's see why this is the case.\n",
    "\n",
    "Remember when we find $\\frac{\\delta J_4(\\theta)}{h1}$, we are trying to calculate the influence of our hidden state at time 1 on our cost function at time 4.  As we see from the diagram below, the influence of $h1$ on $J_4$ is indirect, and thus we'll need the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./backprop-rnn1.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we need to calculate how a nudge of $h_1$ influences $h_2$, and then how a nudge of $h_2$ influences $h_3$ and so on.  We can write this out as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} \\frac{\\delta J_4}{\\delta h_4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate those three derivatives, $\\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} $ individually with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* $h_4 = W_h h_3 + W_x x_4 +b \\rightarrow \\frac{\\delta h_4}{\\delta h_3} = W_h$ \n",
    "* $h_3 = W_h h_2 + W_x x_3 + b  \\rightarrow \\frac{\\delta h_3}{\\delta h_2} = W_h$ \n",
    "* $h_2 = W_h h_1 + W_x x_3 + b \\rightarrow \\frac{\\delta h_2}{\\delta h_1} = W_h$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that $\\frac{\\delta J_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} \\frac{\\delta J_4}{\\delta h_4} = W_h*W_h*W_h \\frac{\\delta J_4}{\\delta h_4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we know, when W_h is small, multiplying these $W_h$ matrices together leads to a vanishing gradient, where our gradient gets drastically smaller as we backpropagate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see occurring is that because: \n",
    "\n",
    "$\\frac{\\delta h_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, if we ask the question, how should $h_1$ be nudged to have an influence on $h_4$, so that we ultimately reduce $J_4(\\theta)$, we see that  $\\frac{\\delta h_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} = W_h*W_h*W_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about the vanishing gradient problem.  As mentioned, a vanishing gradient occurs when our gradient gets drastically smaller as we backpropagate further.  And RNNs are particularly susceptible to vanishing gradients as the hypothesis function repeatedly uses the same weight matrix $W_h$ to calculate the next hidden state.  Because of this, when backpropagating, we repeatedly multiply by $W_h$ for each time step earlier than $t$.  And this means that  when predicting the next word, $w_{t+1}$, our RNNs tend not to incorporate words from earlier timesteps, $w_{t-n}$.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
