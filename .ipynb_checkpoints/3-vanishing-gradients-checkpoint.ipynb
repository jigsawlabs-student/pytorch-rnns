{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this lesson, we'll learn about the vanishing gradient problem.  All neural networks are susceptible to the vanishing gradient problem, but as we'll see RNNs are particularly susceptible to this issue.  Let's dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Vanishing Gradient Problem\n",
    "\n",
    "The vanishing gradient problem is the following:\n",
    "> When determining how to update our parameters through backpropagation, if a local gradient is less than one, this gradient gets drastically smaller as we backpropagate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanishing gradient problem can occur in feedforward neural networks, and convolutional neural networks that we have seen so far.  But with RNNs, we are particularly susceptible to the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs and Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall that with an RNN, the key component is that we are repeatedly calculating a hidden state, where that hidden state is a function both the previous hidden state, our current word embedding, and our weights and biases: $W_h$, $W_e$, and $b$.  We calculate the hidden state with the following formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(t) = \\text{tanh}(W_h h^{t -1} + W_e e^t + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we'll take that final hidden state, and make a prediction as to the next word, and from there determine how to update our parameters after calculating how off our prediction was.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things more concrete, let's say that we are calculating our hidden state for our fourth word at step 4, and this means that we calculate our hidden state according to the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $h_4 = \\text{tanh}(W_h \\cdot h_{3} + W_e \\cdot e_{w4} + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we'll use this hidden state, h_4, to predict our next word $w_5$, and that we calculate our loss at that word as $J_4(w_5, h_4) $ .\n",
    "\n",
    "> The above is just saying that our loss is a function of the next word we are predicting, $w_5$ and the hidden state for the current word $h_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we know we have some loss $J_5$, and our next step is to update our parameters to reduce our loss throuh gradient descent.  Currently, it looks like our parameters that predict $w_5$ are limited to what we in our above formula for $h_4$: $W_h, h^{3}, W_x, x^4$, but really this $h_3$ arises as a function of all of the previous hidden states.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of this visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"h1-with-words.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, our prediction of the next word is (rightfully) influenced by the preceding words through our hidden states.  \n",
    "\n",
    "Now, let's say we want to determine how to update $h_1$ so that we reduce our cost $J_4(\\theta)$.  \n",
    "\n",
    "> Or in other words, we want to update the influence of the the second word \"dog\", in our prediction of the next word, to reduce our cost $J_4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we need to calculate $\\frac{\\delta J_4(\\theta)}{h1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Really technical note**: While we cannot directly change $h_1$, we **can** change $W_h$ which influences $h_1$.  So to determine how to update $W_h$ for the purposes of changing $h_1$, we need to determine $h_1$'s influence on $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating the influence of $h_1$ on $J_4$ becomes a problem, because this equals: \n",
    "\n",
    "$\\frac{\\delta J_4(\\theta)}{h1} = \\frac{\\delta J_4(\\theta)}{h_4}*W_h*W_h*W_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll see why the math works out this way later on.  But for right now, let's just assume this is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point is that we are repeatedly multiplying by $W_h$.  Now let's see why this becomes a problem.\n",
    "\n",
    "As it turns out, this is where our vanishing gradient problem comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how.  Let's set $W_h$ to be the following matrix, and see what occurs as we multiply $W_h*W_h*W_h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5 , 0.01],\n",
       "       [0.3 , 0.5 ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "W_h = np.array([[.5, .01], [.3, .5]])\n",
    "W_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1295 , 0.00753],\n",
       "       [0.2259 , 0.1295 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_h.dot(W_h).dot(W_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we can see is that the further more we multiply by $W_h$, the smaller our numbers get.  And remember, this is the vanishing gradient problem:\n",
    "\n",
    "> When determining how to update our parameters through backpropagation, if a local gradient is less than one, this gradient gets drastically smaller as we backpropagate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see this above.  We multiply by W one time for every difference between our the index of our next word, and our influence word.  \n",
    "> So here we multiply by W 3 times, but this difference could be much larger.\n",
    "\n",
    "And every time we multiply by `W_h`, our gradient becomes smaller and smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it's a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we saw how our vanishing gradient occurs in recurrent neural networks, let's better appreciate why this is a problem.\n",
    "\n",
    "This is a problem because oftentimes, when understanding the meaning of a document, words earlier on be a strong predictor of the meaning of words later down in the sequence.  \n",
    "\n",
    "> And remember that our task with our RNN is to repeatedly predict the next word.\n",
    "\n",
    "For example, we saw the influence of the word dog in our current example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"h1-with-words.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the problem can be way more extreme then just a few words apart.  For example consider one of our Amazon reviews:\n",
    "\n",
    "> This is my favorite coconut water that I have tried.  I have tried a lot of different coconut water, and this is the _____.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction that the next word has a strong chance of being \"best\" is influenced by the word favorite earlier on.  But because the words are 19 steps apart, it would have a small influence in our current formulation of RNNs.\n",
    "\n",
    "Another thing to point out is that a consequence of the vanishing gradient problem is not just that the influence of earlier words is small, but that the influence is drastically smaller than more recent words.  And as we viewed in our example, it's sometimes the earlier words that can be more predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little deeper into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we previously just skipped to the end in specifying that our derivative  $\\frac{\\delta J_4(\\theta)}{h1} = \\frac{\\delta J_4(\\theta)}{h_4}*W_h*W_h*W_h$.  Now let's take some time to see why this is the case.\n",
    "\n",
    "Remember that when we find $\\frac{\\delta J_4(\\theta)}{h1}$, we are trying to calculate the influence of our hidden state at time 1 (associated with dog) on our cost function at time 4.  As we see from the diagram below, the influence from the parameters in $h1$ on $J_4$ is indirect, and thus to calculate the gradient we'll need to use the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./backprop-rnn1.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we need to calculate how a nudge of $h_1$ influences $h_2$, and then how a nudge of $h_2$ influences $h_3$ and so on.  We can write this out as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J_4}{\\delta h_1} =  \\frac{\\delta J_4}{\\delta h_4}  \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the calculation of three derivatives, $\\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} $.  We lay out the formula for each hidden state, and then the related derivative.  Take a moment to go throuh them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* $h_4 = W_h h_3 + W_x x_4 +b \\rightarrow \\frac{\\delta h_4}{\\delta h_3} = W_h$ \n",
    "* $h_3 = W_h h_2 + W_x x_3 + b  \\rightarrow \\frac{\\delta h_3}{\\delta h_2} = W_h$ \n",
    "* $h_2 = W_h h_1 + W_x x_3 + b \\rightarrow \\frac{\\delta h_2}{\\delta h_1} = W_h$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that $\\frac{\\delta J_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} \\frac{\\delta J_4}{\\delta h_4} = W_h*W_h*W_h \\frac{\\delta J_4}{\\delta h_4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we know, when $W_h$ is less than one, multiplying these $W_h$ matrices together leads to a vanishing gradient, where our gradient gets drastically smaller as we backpropagate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see occurring is that because: \n",
    "\n",
    "$\\frac{\\delta h_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, if we ask the question, how should $h_1$ be nudged to have an influence on $h_4$, so that we ultimately reduce $J_4(\\theta)$, we see that  $\\frac{\\delta h_4}{\\delta h_1} = \\frac{\\delta h_4}{\\delta h_3} \\frac{\\delta h_3}{\\delta h_2}  \\frac{\\delta h_2}{\\delta h_1} = W_h*W_h*W_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about the vanishing gradient problem.  As mentioned, a vanishing gradient occurs when our gradient gets drastically smaller as we backpropagate further.  And RNNs are particularly susceptible to vanishing gradients as the hypothesis function repeatedly uses the same weight matrix $W_h$ to calculate the next hidden state.  Because of this, when backpropagating, we repeatedly multiply by $W_h$ for each time step earlier than $t$.  And this means that  when predicting the next word, $w_{t+1}$, our RNNs tend not to incorporate words from earlier timesteps, $w_{t-n}$.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
