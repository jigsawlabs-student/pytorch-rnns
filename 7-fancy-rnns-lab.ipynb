{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we'll practice training our data on our TREC dataset.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To start, change the runtime type to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by loading up the necessary libaries and seeding our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "SEED = 12\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, initialize a field object for the text field, and another one for the label field.  For the TEXT field, tokenize with `spacy` include lengths so that we can pack the tensor later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.include_lengths\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we'll download both a training and test data for the TREC dataset.  Pass through the  TEXT and LABEL fields to specify how to tokenize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 2.60MB/s]\n",
      "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 1.10MB/s]\n",
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, next numericalize the both the TEXT and LABEL fields.   For the text field, download the glove word vectors of length 300 and that were trained on 6 billion words.  Only numericalize the 25000 most frequent words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:50, 2.10MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:28<00:00, 14197.02it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = 25_000, \n",
    "                 vectors = \"glove.6B.300d\", \n",
    "                 unk_init = torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the label field has numericalized the six categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# defaultdict(<function torchtext.vocab._default_unk_index>,\n",
    "#             {'ABBR': 5, 'DESC': 2, 'ENTY': 0, 'HUM': 1, 'LOC': 4, 'NUM': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's bucket our data into batches of size 64.  Set the device equal to a cuda device, and set the batch size equal to 64.  Set, `sort_within_batch` to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size = 64,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the first batch of text from the `train_iterator` by looping through the data and breaking after the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    first_batch_text = batch.text\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `first_batch_text` should be a tuple of the numericalized words and the number of non-padded words for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(first_batch_text)\n",
    "# tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by initializing the following layers of our LSTM model.  First we need to set the embedding layer.  It should have a row for each word in our vocabulary, and a column equal to the length of the vectors.  Also pass through the padding index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes the LSTM layer.  Specify four LSTM layers, that are bidirectional, with a dropout rate of .5.  There should be 200 dimensions for the hidden state.  Then initialize a dropout layer with a rate of .5, that we'll eventually use between our last LSTM layer and our first linear layer.\n",
    "\n",
    "Then create the linear layer, which will be the output layer.  Each neuron of the linear layer should take in the hidden state from the last layer, both the forwards and backwards hidden states.  Then, let's initialize the neural network, and check the result below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(9343, 300, padding_idx = 1)\n",
    "        self.lstm_layer = nn.LSTM(300, 200, num_layers=4, bidirectional=True, \n",
    "                           dropout=.5)\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.fc = nn.Linear(200 * 2, 6)\n",
    "    def forward(self, text, document_lengths):\n",
    "        embedded_batch = self.embedding(text) # torch.Size([713, 64, 100])\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded_batch, \n",
    "                                                    document_lengths, \n",
    "                                                    enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm_layer(packed_embedded)\n",
    "        l2_forwards = hidden[-2,:,:]\n",
    "        l2_backwards = hidden[-1, :, :]\n",
    "        combined_hidden = torch.cat((l2_forwards, \n",
    "                                     l2_backwards), dim = 1)\n",
    "        dropout = self.dropout(combined_hidden)\n",
    "        output_layer = self.fc(dropout)\n",
    "        return F.log_softmax(output_layer, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(9343, 300, padding_idx=1)\n",
       "  (lstm_layer): LSTM(300, 200, num_layers=4, dropout=0.5, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=400, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = LSTM()\n",
    "lstm\n",
    "\n",
    "# LSTM(\n",
    "#   (embedding): Embedding(9343, 300, padding_idx=1)\n",
    "#   (lstm_layer): LSTM(300, 200, num_layers=4, dropout=0.5, bidirectional=True)\n",
    "#   (dropout): Dropout(p=0.5, inplace=False)\n",
    "#   (fc): Linear(in_features=400, out_features=6, bias=True)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, next it's time to fill in the `forward` method.  The method should take in both the text and the document_lengths of each batch.  Then pass the text through the embedding layer, pack the documents to remove the padding from the documents.  Then pass the padded sequence to the LSTM layer.  \n",
    "\n",
    "The LSTM layer returns to us the output, the hidden state and the cell of the LSTM.  From hidden, select the forwards hidden layer and backwards hidden layer, and then concatenate the two states into a vector.  Then apply the dropout to the concatenated hidden state.  Finally, pass the dropout to the linear layer and apply the `log_softmax` to output our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before trying out our forward function, we need to place our model on cuda.  Do so below, and reassign the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then try out the model by making predictions, passing through both the `text` and the `document_lengths`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = None\n",
    "\n",
    "predictions.shape\n",
    "# torch.Size([64, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our predictions are working, copy the glove word embeddings over to the embedding layer of the LSTM.\n",
    "\n",
    "> Replace the vectors associated with the unknown token and pad tokens with a vector of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "lstm.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "lstm.embedding.weight.data[UNK_IDX] = torch.zeros(300)\n",
    "lstm.embedding.weight.data[PAD_IDX] = torch.zeros(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our model.  Use the Adam optimizer, and cross entropy for our loss, as we are perform a multiclass classification problem.\n",
    "\n",
    "Then ensure that the cross entropy loss operates on cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(lstm.parameters())\n",
    "\n",
    "c_e_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "lstm = lstm.to(device)\n",
    "c_e_loss = c_e_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theen define the training loop, printing the loss at each step.  We may need to convert the labels to type `long` when calculating our cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(15):\n",
    "    for batch in train_iterator:\n",
    "        preds = lstm(*batch.text)\n",
    "        loss = c_e_loss(preds, batch.label.to(device).long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, when training is completed, compute the accuracy of the test data.  We'll define the `categorical_accuracy` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's your task to create a list of `accuracies` for each batch of data.  The calculated the accuracy for the entire test set using a weighted sum below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "batch_lengths = []\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        predictions = lstm(*batch.text)\n",
    "        acc = categorical_accuracy(predictions.to(device), batch.label.to(device).long())\n",
    "        accuracies.append(acc.item())\n",
    "        batch_lengths.append(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sum = sum([accuracy*batch_length for accuracy, batch_length in zip(accuracies, batch_lengths)])/sum(batch_lengths)\n",
    "weighted_sum\n",
    "# 0.8699999995231629"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
