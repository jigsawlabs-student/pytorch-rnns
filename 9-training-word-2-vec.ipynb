{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The n-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `word2vec` iterates on the values of vectors to say given one word with certain values, how well can we predict context words. For example, one initial way of doing this could be to say given previous words in a sentence, what is the next word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(w_{t + 1}|w_1,w_2,...w_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine what this would look like in practice however.  For example, `Word2vec is a group of related models`.  If we want to calculate the probability of the word `models` given the phrase `Word2vec is a group of related`, we have the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(w_{t + 1}|w_1,w_2,...w_t) = \\frac{COUNT(Word2vec\\_is\\_a\\_group\\_of\\_related\\_models)}{COUNT(Word2vec\\_is\\_a\\_group\\_of\\_related)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrase `Word2Vec is a group of related ` probably only occurs one time in the entire corpus, so we really won't have enough data to make predictions based on this approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of using all of the previous words in a sentence to predict a word, we can just choose perhaps the word before it to predict the current word.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(w_{t + 1}|w_1,w_2,...w_t) = p(w_{t+1} | w_t) $\n",
    "\n",
    "So using our above example, now we would have: \n",
    "\n",
    "$\\frac{COUNT(of\\_related)}{COUNT(of)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the window of previous words to include more context, with the tradeoff fewer examples to calculate each probability.  This is the n in `n-gram`.  The `n` represents the number of previous words to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Gram and Continuous Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to relax the requirement of previous words, is to consider the surrounding words, both before and after.  \n",
    "\n",
    "####  Continuous bag of words\n",
    "\n",
    "The continuous bag of words model predicts the current word based on the surrounding words.\n",
    "\n",
    "So now if we want to calculate the word related, and were training on the phrase \n",
    "\n",
    "`is a group of related models that are used` \n",
    "\n",
    "With $w= 1$, we have:\n",
    "\n",
    "\n",
    "* $P(related|of)$\n",
    "* $P(related|models)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $w=2$, we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $P(related|of)$\n",
    "* $P(related|models)$\n",
    "* $P(related|group)$\n",
    "* $P(related|that)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skipgram model does the opposite.  It predicts the context words given the current word.  \n",
    "\n",
    "<img src=\"word-2-vec-pw.png\" width=\"40%\">\n",
    "\n",
    "We'll define the following terms:\n",
    "\n",
    "* $w_t = $ current word\n",
    "* $w_{t + j}$ = context word\n",
    "* $c$ = context window size\n",
    "\n",
    "Such that $j\\neq 0$ and $|j| \\leq c $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we have the phrase: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`is a group of related models that are used` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can let the word `related` be our current word, which will be used to predict each of our context words.  \n",
    "\n",
    "So  $w_t = related$ and we try to predict the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $c= 1$, we have:\n",
    "\n",
    "* $P(w_{t-1}) = P(of|related)$\n",
    "* $P(w_{t+1}) = P(models|related)$\n",
    "\n",
    "With $c=2$, we have:\n",
    "\n",
    "* $P(w_{t-1}) = P(of|related)$\n",
    "* $P(w_{t+1}) = P(models|related)$\n",
    "* $P(w_{t-2}) = P(group|related)$\n",
    "* $P(w_{t+2}) = P(that|related)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in other words, for each word in the context window, we try to calculate: \n",
    "\n",
    "* $P(w_{t+j}| w_t)$\n",
    "\n",
    "Such that $j\\neq 0$ and $|j| \\leq c $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a context window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we saw that to predict a word, $w_{t + j}$ in the context window of our current word $w_t$, we can represent this as: \n",
    "\n",
    "$P(w_{t+j}| w_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the probability the entire context window, given a current word.  \n",
    "\n",
    "* $P(group\\_of\\_related\\_models\\_that) = P(group|related)*P(of|related)*P(models|related)* P(that|related)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or written another way, we let $c = 2$, then probability of context window, $cw$, given our current word, $w_t$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(cw) = P(w_{t-2}| w_t)*P(w_{t-1}| w_t) * P(w_{t+1}| w_t) * P(w_{t+2}| w_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which equals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(cw) = {\\displaystyle \\prod_{|j| < c; j \\neq 0}} P(w_{t + j}| w_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to predict an entire corpus.  Now this is simply the probability of each of the context windows occurring.  How many context windows do we have for a given corpus?  Well each word in a corpus gets a context window, so $COUNT(w_t) = COUNT(cw)$.  \n",
    "\n",
    "And calculating the probability of an entire corpus, is calculating the probability of each context window -- one for each word in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(corpus) = P(cw_0)*P(cw_1)*P(cw_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(corpus) = {\\displaystyle \\prod_{0 -> len(corpus)}} P(cw)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, via substitution we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(corpus) = {\\displaystyle \\prod_{t=1}^T}  {\\displaystyle \\prod_{|j| < c; j \\neq 0}} P(w_{t + j}| w_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is the likelihood of the entire corpus, and the task of word2vec is to choose parameters of theta such that it maxes the likelihood of our corpus.  In other words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$argmax(\\theta) {\\displaystyle \\prod_{t=1}^T}  {\\displaystyle \\prod_{|j| < c; j \\neq 0}} P(w_{t + j}| w_t; \\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with a big pile of text, and continuous text (sentences).  So begin with a corpus.\n",
    "2. And every word is represented by a random vector\n",
    "3. Then say, here is a word in the text, look at the words around it, and want the word in the middle, and want it to predict the word in the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the next word, to see what occurs around banking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a long corpus of words, and so go through.  We want to maximize the likelihood of words around given the vector representation of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Word2Vec](https://rohanvarma.me/Word2Vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[stanford notes](https://cs224d.stanford.edu/lecture_notes/notes1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[d2l.ai](https://d2l.ai/chapter_preliminaries/probability.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[babylon - word and context words](http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[meap book ](https://www.amazon.com/Natural-Language-Processing-Action-Understanding/dp/1617294632/ref=sr_1_3?keywords=Natural+Language+Processing+in+Action&qid=1573581726&sr=8-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
