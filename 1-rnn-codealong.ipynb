{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Code Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll work through building the hypothesis function for a recurrent neural network in Python.  In doing so, we'll explore how an RNN generates and uses hidden state for each word in a document.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Initial Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at our diagram for a recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Rnn-diagram-no-pred.png\"  width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we begin with an index that represents each word.  Then from the index we get the related embedding.  Then we produce a hidden state by taking the embedding and multiplying by weights, and getting the previous hidden state and also multiplying by weights.  We'll combine the two computations with addition.  Ok, let's start implementing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by loading up some libraries, and let's initialize a word vector through the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7056],\n",
       "        [ 0.6741],\n",
       "        [-0.5454],\n",
       "        [ 0.9107]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(15)\n",
    "e_1 = torch.randn(4, 1)\n",
    "e_1\n",
    "# dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we'll say that the above represents the word dog, through a word embedding of length four."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also initialize an inital hidden state $h_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "h_0 = torch.randn(1, 1)\n",
    "h_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we know, we'll produce the next hidden state with the function $H_t = e_t \\cdot w_e +  h_{t-1}\\cdot w_h$.  So we'll need weights to associate with both the embedding and the hidden state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2138]]),\n",
       " tensor([[-1.3780],\n",
       "         [-0.0546],\n",
       "         [ 0.4515],\n",
       "         [ 0.7858]]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(12)\n",
    "w_h = torch.randn(1, 1)\n",
    "w_e = torch.randn(4, 1)\n",
    "w_h, w_e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We initialize a single number for our starting hidden state, and a w_e with length equal to the length of our embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get to the next hidden state by multiplying the embedding and the hidden state by their respective weights.\n",
    "\n",
    "> `mm` stands for matrix multiplication.  We leave out the addition of a bias term for simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4288]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.mm(h_0, w_h) + torch.mm(e_1.T, w_e)\n",
    "#        1x1, 1x1            1x4.    4x1 = 1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is the hidden state that represents the current word, \"dog\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move the operation above to a function, so we can reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_t(e_t, w_e, h_t_prev, w_h):\n",
    "    return torch.mm(h_t_prev, w_h) + torch.mm(e_t.T, w_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4288]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t(e_1, w_e, h0, w_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we now have our formula to calculate the next hidden state.  Now let's take a sequence of word embeddings, and predict the next hidden state for each word embedding, $e_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3029,  0.2045, -0.9202, -0.8919,  0.2516],\n",
       "        [ 0.9675, -0.6870,  0.9042,  0.3286, -0.0742],\n",
       "        [ 0.1414, -1.2538, -0.3456, -0.2211, -0.7043],\n",
       "        [ 0.3368,  0.0064,  0.2326,  0.9527, -0.4139]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dog jumped over a\n",
    "E = torch.randn(4, 5)\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that we now have a matrix where each column represents a word in the sequence above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how to do we compute the hidden state for this sequence?  Well, we should probably loop through the data, and in each iteration calculate the hidden state and save the hidden state to be used in the next iteration.\n",
    "\n",
    "We'll save our hidden states in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = [h_0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll initialize this list with our first hidden state, `h_0`.  Now let's produce our hidden state at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e_t in E.T:\n",
    "    e_t = e_t.T.unsqueeze(0)\n",
    "    \n",
    "    prev_hidden = hidden_states[-1]\n",
    "    hidden_prod = torch.mm(prev_hidden, w_h)\n",
    "    embed_prod = torch.mm(e_t, w_e)\n",
    "    new_h = hidden_prod + embed_prod\n",
    "    hidden_states.append(new_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1117]]),\n",
       " tensor([[-1.4958]]),\n",
       " tensor([[-0.4856]]),\n",
       " tensor([[1.3492]]),\n",
       " tensor([[1.5714]]),\n",
       " tensor([[-1.3220]])]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are six total hidden states.  There's one for each of the words, $e_1 ... e_t$, and plus the initial hidden state $h_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Oriented RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, if we prefer, we can move this same process to Pytorch.  This is what is would look like.  In the code below we simply initialize a linear layer for both our word embedding and our hidden state.  Our word embedding is a vector of length four, one row for the length of our word vectors.  And the hidden state is a 1x1 matrix.\n",
    "\n",
    "We initialize an a list to store our hidden states, initializing an initial hidden state with a random number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_states = [torch.randn(1, n_neurons)]\n",
    "        self.We = nn.Linear(n_inputs, n_neurons) # 4 X 1\n",
    "        self.Wh = nn.Linear(n_neurons, n_neurons) # 1 X 1\n",
    "    \n",
    "    def forward(self, e):\n",
    "        \n",
    "        prev_hidden = self.hidden_states[-1]\n",
    "        \n",
    "        hidden_mult = self.Wh(prev_hidden) \n",
    "        embed_mult = self.We(e) # 4 X 1\n",
    "        hidden_state = torch.tanh(hidden_mult + embed_mult)\n",
    "        self.hidden_states.append(hidden_state)\n",
    "        return hidden_state\n",
    "        #1x4 4x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then in the forward function, we select the previous hidden state and multiply it by our Wh, we take our word embedding, `e` and multiply it by the associated weights `We`.  Then we apply our non-linearity, here the tanh function because that's what Pytorch uses, and we end by appending in our hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's try it out.  \n",
    "\n",
    "We initialize our RNN by specifying the number of dimensions of each word embedding, and that our hidden state will be of length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (We): Linear(in_features=4, out_features=1, bias=True)\n",
       "  (Wh): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(4, 1)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we loop through every column in matrix of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e_t in E.T:\n",
    "    rnn(e_t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see that we get the following hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.1709]]),\n",
       " tensor([[-0.0148]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.0774]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.1840]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.2288]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.3159]], grad_fn=<TanhBackward>)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now currrently we are setting up our rnn so that our hidden state is a single number.  But we can increase the number of neurons on our $W_h$, and increase the number of columns in our hidden state.  We do so simply by increasing the number of neurons is $W_h$ -- here we increase it to 2.  Notice that we also set our initial hidden state to be of length 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (We): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (Wh): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(4, 2)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it.  Everything else is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e_t in E.T:\n",
    "    rnn(e_t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.8746, -1.1934]]),\n",
       " tensor([[0.7477, 0.7169]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.7183, 0.0297]], grad_fn=<TanhBackward>),\n",
       " tensor([[-0.0921, -0.0526]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.3248, 0.5440]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.6136, 0.2024]], grad_fn=<TanhBackward>)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the hidden state is called the hidden dimension, and we can use a length in the hundreds to capture many features from our text. All this involves is increasing the size of our weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we translated our explanation of an RNN into code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Rnn-diagram-no-pred.png\"  width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that we can initialize our first hidden state as a vector length of our choosing, and then to produce a hidden state for each word, we multiply a set of weights by the previous hidden state, and multiply the word embedding by a different set of weights. After the matrix multiplication, we simply add the results.  In the next lesson, we'll see how to implement our RNN in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[RNN Pytorch](https://medium.com/dair-ai/building-rnns-is-fun-with-pytorch-and-google-colab-3903ea9a3a79)\n",
    "\n",
    "[NN from Scratch Pytorch](https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
